{
  "data": "/data/downstream/ds002105",
  "frac_val_per_dataset": 0.05,
  "n_val_subjects_per_dataset": 3,
  "n_test_subjects_per_dataset": 9,
  "n_train_subjects_per_dataset": 11,
  "parcellation_dim": 1024,
  "pretrained_model": "results/models/upstream/GPT_lrs-4_hds-12_embd-768_train-CSM_lr-0005_bs-192_drp-01/model_final/pytorch_model.bin",
  "embedding_dim": 768,
  "num_hidden_layers_embedding_model": 1,
  "tr_max": 300,
  "tr_precision": 0.2,
  "freeze_embedder": false,
  "num_hidden_layers_unembedding_model": 1,
  "freeze_unembedder": false,
  "architecture": "GPT",
  "num_hidden_layers": 4,
  "num_attention_heads": 12,
  "intermediate_dim_factor": 4,
  "hidden_activation": "gelu_new",
  "n_positions": 512,
  "freeze_decoder": false,
  "freeze_decoder_without_pooler_heads": false,
  "resume_from": null,
  "training_style": "decoding",
  "decoding_target": "task_label.pyd",
  "num_decoding_classes": 26,
  "training_steps": 10000,
  "validation_steps": 1000,
  "test_steps": 1000,
  "per_device_training_batch_size": 64,
  "per_device_validation_batch_size": 64,
  "optim": "adamw_hf",
  "learning_rate": 0.0001,
  "warmup_ratio": 0.01,
  "weight_decay": 0.1,
  "adam_beta_1": 0.9,
  "adam_beta_2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "lr_scheduler": "linear",
  "sample_random_seq": true,
  "seq_min": 10,
  "seq_max": 50,
  "bert_seq_gap_min": 1,
  "bert_seq_gap_max": 5,
  "masking_rate": 0.2,
  "dropout": 0.1,
  "autoen_teacher_forcing_ratio": 0.5,
  "log_dir": "results/models/downstream/ds002105\\GPT_lrs-4_hds-12_embd-768_train-decoding_lr-0001_bs-64_drp-01_2022-11-16_12-16-49",
  "log_every_n_steps": 1000,
  "run_name": "GPT_lrs-4_hds-12_embd-768_train-decoding_lr-0001_bs-64_drp-01_2022-11-16_12-16-49",
  "wandb_mode": "disabled",
  "wandb_project_name": "learning-from-brains",
  "seed": 1234,
  "set_seed": true,
  "fp16": true,
  "deepspeed": null,
  "local_rank": -1,
  "num_workers": 0,
  "plot_model_graph": false,
  "smoke_test": false,
  "bold_dummy_mode": false,
  "do_train": true
}